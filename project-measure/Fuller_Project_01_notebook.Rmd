---
title: 'Notebook for Project 01'
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 4
    fig_caption: true
    number_sections: true
params:
  knitChunkSetEcho: TRUE
  knitChunkSetWarning: TRUE
  knitChunkSetMessage: TRUE
  knitChunkSetCache: TRUE
  knitChunkSetFigPath: "graphics/"
---

```{r setup, include=FALSE}
# I am now setting parameters in YAML header, look above
knitr::opts_chunk$set(echo = params$knitChunkSetEcho);
knitr::opts_chunk$set(warning = params$knitChunkSetWarning);
knitr::opts_chunk$set(message = params$knitChunkSetMessage);

# ... just added ... take a look at how this builds ... you now have your raw files ...
knitr::opts_chunk$set(cache = params$knitChunkSetCache);
knitr::opts_chunk$set(fig.path = params$knitChunkSetFigPath);

# knitr::opts_chunk$set(background = "#981E32"); # only works on *.Rnw
   
# fig.show ... animate ... 

# CRIMSON ... #981E32
# GRAY ... #53565A


# we don't want scientific notation
options(scipen  = 999);

library(devtools);
library(humanVerseWSU);
packageVersion("humanVerseWSU");  # ‘0.1.4’+
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";

library(parallel);
parallel::detectCores(); # 16 # Technically, this is threads, I have an 8-core processor 
```


# Source Functions

```{r humanVerse}

require(devtools);
install_github("MonteShaffer/humanVerseWSU/humanVerseWSU"); 
require(humanVerseWSU);

require(pvclust); # Hclust
require(psych);require(scatterplot3d);require(rgl); # EFA
require(REdaS); require(nFactors);# EFA
require(parallel);require(mvnfast); # PCA
require(scatterplot3d); require(rgl); # PCA 3-d plot

library(devtools);
my.source = "local";
github.path = "https://raw.githubusercontent.com/fullerharrison/WSU_STATS419_FALL2020/";
source_url(paste0(github.path, "master/functions/libraries.R"))

```

# Make Folders

```{r}
path.to.project = "/Users/harrison.fuller/OneDrive - Washington State University (email.wsu.edu)/Classes/STATS 419/WSU_STATS419_FALL2020/project-measure/";

path.graphics = paste0(path.to.project,"graphics/");

createDirRecursive(path.graphics);

datasets.path = "/Users/harrison.fuller/OneDrive - Washington State University (email.wsu.edu)/Classes/STATS 419/WSU_STATS419_FALL2020/project-measure/datasets/";

# Path to file
measure.file2 = paste0(datasets.path,"pipe-format/measure-7db2882a91ebc25c142bfbaaebb1b1ee.txt");

measure = read.csv(measure.file2, header=TRUE, quote="", sep="|");
  measure[, c(27:31, 33:36)] <- lapply(X =measure[, c(27:31, 33:36)], FUN = as.factor);
  measure[, -c(1:3, 27:37)] <- scale(measure[, -c(1:3,27:37)]);
    measure; str(measure);

# Transpose 
cols = colnames(measure);
rows = measure$person_id;

df = removeColumnsFromDataFrame(measure,"person_id");
df.t = transposeMatrix(df);
colnames(df.t) = rows;

measure.t = as.data.frame(df.t);
measure.t; str(measure.t)
```


# Correlations

```{r chunck-correlations}

# Numbers from data
X.corr <- round( cor(measure[, c(4:26)]), digits=2);
X.corr;

# Visualization
corrplot::corrplot( cor(measure.corr));

# Not a lot of data and incomplete 
# 
```

# Heriearchal Clustering


We are now going to apply distance to aggregate multivariate data.  Recall that typically we refer to a data frame based on its rows and columns.  Generally, the rows represent observations and the columns represent features.

As we try to aggregate data, we need to ask:  are we aggregating the rows or the columns?  Why?  So let's look at measure data.

## Hclust

```{r chunk-hierarchal cluster}

X <- removeColumnsFromDataFrame(measure, 
                                c("data_collector", "person_id", "side", "units", "writing", "eye", "eye_color", "swinging", "age", "gender",  "quality", "minutes", "ethnicity", "notes"))
# Cluster by person
X.dist <- dist(X);
X.dist;

# Cluster by feature
X.dist.t <- dist( t(X));
X.dist.t;

# Method and distance formula is arbitrary
# 
methods = c("complete", "average", "single", "median", "centroid", "ward.D", "ward.D2", "mcquitty");

for(method in methods)
  {
  time.start = Sys.time();  
      X.hclust = hclust( X.dist, method=method);
  plot(X.hclust);
  time.end = Sys.time();
  elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
  print(paste0(elapse, " secs to complete method ... ", method));
}



```
## Monte

```{r}
source_url( paste0(path.github, "humanVerseWSU/R/functions-EDA.R") ); 

Xs.hclust.8 = perform.hclust(Xs, n.groups = 8, pvclust.parallel = TRUE); 

```

## Pvclust

```{r chunk-hierarchal pv cluster}
require(pvclust);

for(method in methods)
  {
  time.start = Sys.time();  
      X.pvclust = pvclust (t(X), method.hclust=method);
  plot(X.pvclust);
      pvrect(X.pvclust);
  time.end = Sys.time();
  elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
  print(paste0(elapse, " secs to complete method ... ", method));
}
```

# K-means Cluster

## JV

```{r k-means}


# first a descriptive plot by country
stars(X, len = 0.5, key.loc=c(12,2), draw.segments = TRUE);
# stars need a better palette
palette(rainbow(12, s = 0.6, v = 0.75));
# redraw 
stars(X, len = 0.5, key.loc=c(12,2), draw.segments = TRUE);



# for the faint of heart ...
X.kmeans = kmeans(X, 3);  # default algorithm
stars(X.kmeans$centers, len = 0.5, key.loc = c(4, 3),
        main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=3", draw.segments = TRUE);

print(X.kmeans);

    membership = as.data.frame( matrix( X.kmeans$cluster, ncol=1)) ;
    
    rownames(membership) = protein$Country;

    membership;

    print( table(membership) ) ; 
```

## Basic Plots

```{r}
########################################

# we can't easily plot all of the dimensions, since it is more than 2-D or 3-D ... so we can "cheat" and apply some PCA techniques to data-reduce to two dimensions ...

require(factoextra);   # install.packages("factoextra", dependencies=TRUE);

# this approach uses ggplot2 (not plot)
library(ggplot2);   # install.packages("ggplot2", dependencies=TRUE);
fviz_cluster(X.kmeans,data=X);
fviz_cluster(X.kmeans, data=X, geom="point");
# assumes the cluster is normal?
fviz_cluster(X.kmeans, data=X, geom="point", ellipse.type = "norm");

# try ?fviz_cluster
```

## Varsity

```{r, chunck-kmeans-steroids}
# now for a "FOR LOOP ON STEROIDS"
# a variadic approach ...

path.loop.graphics = paste0(path.graphics,"forloops/");

createDirRecursive(path.loop.graphics);

print(path.loop.graphics);
# maybe open this folder before you run this code ... 

wss.png.list = c();

n.Countries = dim(X)[1];
n.Features = dim(X)[2];

K = 2:6;  # let's try these choices for our "k"
n.K = length(K);

algorithms = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen");
n.A = length(algorithms);

# a nested for loop, order matters, I want to study each algorithm as I loop over the special K's

membership = as.data.frame( 
                matrix(0, nrow=n.Countries, ncol=n.K*n.A) );
  rownames(membership) = protein$Country;

m.col.names = c(); # don't know these yet ...

j=0;
for(algorithm in algorithms)
  {
  wss = numeric(n.K); # total within-sum-of-squares
  times = numeric(n.K);
  i = 0;
  for(k in K)
    {
    print( paste0("algorithm: ", algorithm, " ",
           paste0(rep("=",times=k),collapse=""), "> ",k));
    j = 1 + j;
    i = 1 + i;
    time.start = Sys.time(); 
    # I am appending to kmeans some previous results, so:
      #X.kmeans = kmeans(X[,n.Features], k, algorithm=algorithm);
    X.kmeans = kmeans(X, k, algorithm=algorithm);
      # str(X.kmeans);
      
      # key.loc probably needs to be adjusted per kmeans instance ... once you have a final answer, for your final writeup, you would really take the time to make the graph look perfect ... we are in a sandbox ... the "legend or key" may cover up one of your results.


stars(X.kmeans$centers, len = 0.5, key.loc = c(4, 3),
            main = paste0("Algorithm: ", algorithm, 
              " \n Stars of KMEANS=",k), draw.segments = TRUE);
   
    star.file = paste0(path.loop.graphics,"algo-", paste0(tolower(algorithm),collapse=""),"_k-",k,"_stars_.png");  # could also do PDF
    
################### writes image as png to file    
png(star.file);        
    stars(X.kmeans$centers, len = 0.5, key.loc = c(4, 3),
            main = paste0("Algorithm: ", algorithm, 
              " \n Stars of KMEANS=",k), draw.segments = TRUE);
dev.off();
###################  
    
################### writes image as pdf to file    
pdf( gsub(".png",".pdf",star.file, fixed=TRUE) );        
    stars(X.kmeans$centers, len = 0.5, key.loc = c(4, 3),
            main = paste0("Algorithm: ", algorithm, 
              " \n Stars of KMEANS=",k), draw.segments = TRUE);
dev.off();
###################     
# obviously, subfolders may help ... this is a demo ...    
    
        stars(X.kmeans$centers, len = 0.5, key.loc = c(4, 3),
            main = paste0("Algorithm: ", algorithm, 
              " \n Stars of KMEANS=",k), draw.segments = TRUE);
    
    
    # map of clusters back to countries
    my.key = paste0(algorithm,".",k);
    m.col.names = c(m.col.names, my.key);
    
    membership[,j] = X.kmeans$cluster;
    
    print( table(X.kmeans$cluster) ) ;  
    #plot(X, col=X.kmeans$cluster);
    #points(X.kmeans$centers, col = 1:2, pch = 8, cex = 2);
  
    time.end = Sys.time();
    elapse = sprintf("%.3f", 
                as.numeric(time.end) - as.numeric(time.start));
    
# don't include this step in "timings" ... slows things down...
# rendering?
    
pca.clust.file = gsub("_stars_","_clusters_",star.file, fixed=TRUE);



################### writes image as png to file    
png(pca.clust.file);        
    fviz_cluster(X.kmeans, data=X, 
                geom="point", ellipse.type = "norm");
dev.off();
###################  
    
################### writes image as pdf to file    
pdf( gsub(".png",".pdf",pca.clust.file, fixed=TRUE) );        
    fviz_cluster(X.kmeans, data=X, 
                geom="point", ellipse.type = "norm");
dev.off();
###################   

print("fviz_cluster is not rendering inside of for loop");
fviz_cluster(X.kmeans, data=X, 
                geom="point", ellipse.type = "norm");
    
    wss[i] = X.kmeans$tot.withinss;
    times[i] = elapse;
    }
  #print(times);
  #print(wss);
  base::plot(K,times, main=paste0("[Deceptive] Time in seconds: ",algorithm), type="b"); # deceptive ... you need to run the code once, look at the ranges then rerun with valid ranges ...
  # if you want to compare graphs visually, they should have the same axes.
  # A good graph has a title, a xlab, a ylab
  # Final writeup should have a nice caption for each image ...
  
  base::plot(K,times, ylim=c(0, 0.1), main=paste0("Time in seconds: ",algorithm), type="b");
  
  wss.file = paste0(path.graphics,"algo-", paste0(tolower(algorithm),collapse=""),"_k-",k,"_wss_.png"); # this is one folder up ...
  
  wss.png.list = c(wss.png.list,wss.file);
################### writes image as png to file    
png(wss.file);        
    base::plot(K,wss, ylim=c(0,3000), main=paste0("Total WSS: ",algorithm), type="b");
dev.off();
###################  
    
################### writes image as pdf to file    
pdf( gsub(".png",".pdf",wss.file, fixed=TRUE) );        
    base::plot(K,wss, ylim=c(0,3000), main=paste0("Total WSS: ",algorithm), type="b");
dev.off();
################### 

  # knitr will probably automate this process for us, more to follow: <https://www.r-bloggers.com/2014/01/fast-track-publishing-using-knitr-exporting-images-for-sharing-and-press-part-iii/>  
  
  # want to learn more about plot:  <https://statisticsglobe.com/plot-in-r-example>
  
  
  }


colnames(membership) = m.col.names;
membership;


# rendering?  ... BUGGY ...
print("fviz_cluster renders outside of for loop");
k = 3; # my final answer?
X.kmeans = kmeans(X, k);
fviz_cluster(X.kmeans, data=X);

k = 4; # my final answer?
X.kmeans = kmeans(X, k);
fviz_cluster(X.kmeans, data=X);

# you may notice that it is changing cluster groups and boundaries at times ...


```

K-means is a nice approach because it is computationally efficient.  It is fast!  We don't notice the speed compared to `hclust` yet because the data sets are very small.

<https://youtu.be/esmzYhuFnds?t=985> I would suggest watching from 16:25 to 23:30 ... especially if you are CS-minded!

The problem with kmeans is the results may change based on the initial seeds.  Here are two iterations I ran with the default algorithm.  Do they look the same as yours?

## Stopping rule: what is an ideal "k"?

The `wss` keeps going down as you can see in the appropriate plots.  A general "rule of thumb" is called the elbow-rule of the Scree plot.

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/algo-hartigan-wong_k-6_wss_.png" style="border: 2px black solid;" />
<div> &#8662; **K-means (wss): Hartigan-Wong** &#8663; </div>

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/algo-lloyd_k-6_wss_.png" style="border: 2px black solid;" />
<div> &#8662; **K-means (wss): Lloyd** &#8663; </div>

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/algo-forgy_k-6_wss_.png" style="border: 2px black solid;" />
<div> &#8662; **K-means (wss): Forgy** &#8663; </div>

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/algo-macqueen_k-6_wss_.png" style="border: 2px black solid;" />
<div> &#8662; **K-means (wss): MacQueen** &#8663; </div>


Also think about what we did previously with `hclust` on this same data.

The library(factoextra) has `hkmeans` and `hcuts` that may work.  I would always be cautious of a black-box solution, look at the graphics, and think about what you want to accomplish.

The `for loop` approach gives me the raw content, now I may want to review the image types side-by-side.

<https://www.zevross.com/blog/2017/06/19/tips-and-tricks-for-working-with-images-and-figures-in-r-markdown-documents/>

To aggregate by country, I am reviewing what is available to me, and am concluding that:

* Use the default algorithm

* For this data set, 3-4 appears to be is a nice choice for `k`

* I should define my centers to create `replicable` final results.

* I may try Poland, West Germany, Romania as my 3 centers (See Figure `K-means: 3b`).


## K-means (final answer)

It is reasonable to pick 3 or 4 as your final answer.  Review the information, and pick one that you can defend.  A `4` gives you more variability but may not add much.  You decide.  Maybe you think `wss` clearly concludes `3`.  Decision making and arriving at a definitive conclusion is an important part of data analytics.  Just be able to defend your choice.

```{r, chunck-kmeans-conclusion}
protein$Country;
# I could try and pass in the centers
# myCenters = matrix( c(X[16,], X[24,], X[18,]), nrow=3);  # This is not 2-D, or 3-D, but 9-D!
# Since we have 9-D, passing in nearest center Poland is not working ...
# X.kmeans = kmeans(X, myCenters);
# East Germany in 2-D (PCA transform) looks closer in distance to the "WEST"; however, in all 9-D it is closest in distance to its assigned cluster.


# this line needs to be changed ... 3 or 4 ?
myFinalAnswer = 3;  # 3 or 4, unless you want to defend 8 ...


X.kmeans = kmeans(X, myFinalAnswer, nstart=100);
# default algorithm, notice increase starts will do more "initial sampling" and find the best one (based on wss).  # I only have 25 countries, and I need to choise 3 ... 2300 would give me all options ? <https://www.calculatorsoup.com/calculators/discretemathematics/combinations.php>

# maybe move key.loc value to make graph look nice ...
stars(X.kmeans$centers, len = 0.5, key.loc = c(4, 3),
        main = paste0("Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=",myFinalAnswer), draw.segments = TRUE);

print(X.kmeans);

    membership = as.data.frame( matrix( X.kmeans$cluster, ncol=1)) ;
    
    rownames(membership) = protein$Country;

    membership;

    print( table(membership) ) ; 

fviz_cluster(X.kmeans,data=X);

# will this be replicable? ...

```
You will need to populate this notebook based on the idea of "zero to hero" discussed as Data Provenance.

You will have a data set coming in.

You need to formulate some initial Research Questions that would interest you.  The power of "one" suggests there is an overall research question that may have sub-questions.

- e.g., Body Proportions and Artistic Renderings
- e.g., NBA Basketball Players vs Our Sample
- e.g., Studies of Covariates on various aspects of the body measurements

Notice the above are three example topics to be discovered, not an actual research question with sub-questions.  Given the data (and possibly external auxiliary data), what would you enjoy researching?  What would you find intriguing?



```{r, chunk-rgl-example}

# EASTER

library(rgl);

# Something like this, but scaled, not animated, and in 3-D:  
# http://md5.mshaffer.com/WSU_STATS419/_images_/stick_figure.gif

```



[+5 EASTER]  Using RGL or some other 3-D plotting tool, build a stick-figure based on your individual body measurements.  We assume the x-axis starts at the center of the eyes and moves laterally: looking right to the end of your outstretched fingertips is one direction; looking left to the end of your outstretched fingertips in another direction.  The y-axis would be the vertical axis from your feet to your head.  The z-axis would be forward-backward with zero being directly between your two feet (placed 'arm-pit' width apart), half of your shoe size being in the positive direction, the other half in the negative direction.  We could imaging z=0 running directly through your body (kinda like your spine and through your head).  Use line segments to connect parts, and a large point (or circle) where segments connect.  Use a rectangularish-shape for the head if a 3-D ellipsoid is too difficult).

https://en.wikipedia.org/wiki/Anatomical_terms_of_location

- x: lateral (right/left)
- y: superior/inferior
- z: anterior/posterior





# Exploratory Factor Analysis (EFA)

## Check Conditions of Multivariate Reduction

Remember, during "exploration" there are really not any significant constraints.  In the previous notebook, I showed a few "normality" overlays of the PCA graphs for the countries (the elliptical forms). 

Some call these "diagnostic tests"

### KMO test

"The **Kaiser-Meyer-Olkin Measure of Sampling Adequacy** is a statistic that indicates the proportion of variance in your variables that might be caused by underlying factors. High values (close to 1.0) generally indicate that a factor analysis may be useful with your data. If the value is less than 0.50, the results of the factor analysis probably won't be very useful."

```{r, chunck-check-kmo}


performKMOTest(X = scale(t(X)))
# this is the standard correlation matrix
Xs.corr = cor(X);

# library(KMO); # install.packages("KMO", dependencies=TRUE);  # not available for R == 4.0.2

require(REdaS); # install.packages("REdaS", dependencies=TRUE);

# https://www.rdocumentation.org/packages/REdaS/versions/0.9.3/topics/Kaiser-Meyer-Olkin-Statistics

```

### Sphericity test

"**Bartlett's test of sphericity** tests the hypothesis that your correlation matrix is an identity matrix, which would indicate that your variables are unrelated and therefore unsuitable for structure detection. Small values (less than 0.05) of the significance level indicate that a factor analysis may be useful with your data."

<https://www.ibm.com/support/knowledgecenter/SSLVMB_23.0.0/spss/tutorials/fac_telco_kmo_01.html>


```{r, chunck-check-bartlett}

# https://www.statology.org/bartletts-test-of-sphericity/
# Bartlett’s Test of Sphericity is not the same as Bartlett’s Test for Equality of Variances.

# this is the standard correlation matrix
Xs.corr = cor(X);

require(psych); # install.packages("psych", dependencies=TRUE);

Xs.bartlett = cortest.bartlett(Xs.corr, n = nrow(X));

str(Xs.bartlett);

# alpha level
alpha = 0.05;

if(Xs.bartlett$p.value < alpha)
  {
  print(paste0("Bartlett's test of sphericity ... pvalue < alpha ... ", Xs.bartlett$p.value , " < ", alpha, " ... \n CONCLUSION: we believe this data is likely suitable for factor analysis or PCA"));
  
  
  } else {  
          print("Oh snap!"); 
          print("To put this in layman's terms, the  variables in our dataset are fairly uncorrelated so a data reduction technique like PCA or factor analysis would have a hard time compressing these variables into linear combinations that are able to capture significant variance present in the data. <https://www.statology.org/bartletts-test-of-sphericity/>");
          }

# will be available in humanVerseWSU ... 0.1.4.1 (coming soon) ... 
# performBartlettSphericityTest(Xs);
# performBartlettSphericityTest(Xs.corr, n = nrow(Xs));
# myMeasure ... 

```

## How many factors

In this section, I will review various approaches to attempt to arrive at a conclusion of the number of factors (without merely looking at a scree plot like we did in `kmeans (wss)`.

### Old School: Very Simple Structure (VSS) 

```{r, chunck-nfactors-vss}

# pick a maximum number to examine
maxFactors = 8;  # I have 60 variables

Xs.vss = vss(Xs, n = maxFactors); # could also input Xs.corr, but you would want to include n.obs = nrow(Xs)

#str(Xs.vss);

Xs.vss.dataframe = cbind(1:maxFactors,Xs.vss$vss.stats[,c(1:3)]);

colnames(Xs.vss.dataframe) = c("Factors","d.f.","chisq","p.value");

Xs.vss.dataframe;

# we have so much data, it is saying we could use many different factors ...

# the choice for "4" seems to mean the "gain of a new factor" is minimal.

# https://personality-project.org/r/vss.html

# `__student_access__\sample_latex_files\Multivariate-2009\MonteShaffer_Stats519_HW5.pdf` # See HW5, pg 11 of a custom `multifactanal` table I constructed to use multiple criteria to assess optimal factors ...


```

### Scree: Using eigenvalues from Correlation 

```{r, chunck-nfactors-eigen}

Xs.corr.eigen = eigen(Xs.corr);  

library(nFactors);  # install.packages("nFactors", dependencies=TRUE);

# Basic
plotuScree(Xs.corr.eigen$values);
abline(h = 1, col="blue");

Xs.corr.eigen$values[Xs.corr.eigen$values > 1];
# technically 11 are greater than 1.
# 5-6 would seem reasonable based on what we see


# Steroids
nResults = nScree(eig = Xs.corr.eigen$values,
              aparallel = parallel(
                              subject = nrow(Xs), 
                              var = ncol(Xs) )$eigen$qevpea);

plotnScree(nResults, main="Component Retention Analysis");
# This is suggesting "6" based on Parallel Analysis and Optimal Coordinates ...

str(nResults);


# howManyFactorsToSelect(Xs);
```


### Eigenvalues > 1 plus parallel sampling

```{r, chunck-nfactors-parallel}
# I could loop over the data like I did in the `kmeans` notebook, but we can now consider functions that already do that ...

library(psych);  # install.packages("psych", dependencies=TRUE);

library(GPArotation);  # install.packages("GPArotation", dependencies=TRUE);

Xs.parallel = fa.parallel(Xs, fm = "minres", fa = "fa");

str(Xs.parallel);

# This is suggesting between 5-6

```


## Analysis with chosen factors (let's say 5)

```{r, chunck-factor5-summary}




round( psych::describe(X), digits=5);

Xs.factanal.5 = factanal(X, factors=5, rotation='none');
    # this uses "mle" method ...
    # ## rotation## #
    # varimax = assumes data is independent
    # promax = does a transform
    # none = nothing
# Xs.factanal.5 = factanal(covmat=Xs.corr, n.obs=nrow(Xs), factors=5, rotation='varimax');


# Uniqueness
head(Xs.factanal.5$uniquenesses);
# "Uniqueness" shows what???


# Map of Variables to Factors (Loadings)
print(Xs.factanal.5$loadings, digits=2, cutoff=0.25, sort=FALSE);

plot(Xs.factanal.5$loadings[,1:2], type="n");
text(Xs.factanal.5$loadings[,1:2],labels=names(Xs),cex=.7) # add variable names


print("Cool 3D graphs start here");
library(scatterplot3d); # install.packages("scatterplot3d", dependencies=TRUE);
library(rgl); # install.packages("rgl", dependencies=TRUE);





pchs = numeric(ncol(Xs));
  pchs[1:30] = 16;  # self
  pchs[31:60] = 17;  # other

# https://www.sessions.edu/color-calculator/
c.choices = c("steelblue", "#b46e46");
colors = character(ncol(Xs));
  colors[1:30] = c.choices[1];  # self
  colors[31:60] = c.choices[2];  # other


Xs.sp3d = scatterplot3d(Xs.factanal.5$loadings[,1:3],
              pch=pchs, color=colors,
              grid=TRUE, box=TRUE,
              type="p",
              angle=22
                );
legend(Xs.sp3d$xyz.convert(1.5, -0.75, 1),
        legend = c("Self","Other"),
        col =  c.choices,
        text.col = c.choices,
        pch = 16:17,
        bty = 'n'
      );  
  
# radius
rs = numeric(ncol(Xs));
  rs[1:30] = 0.05;  # self
  rs[31:60] = 0.03;  # other


# http://www.sthda.com/english/wiki/a-complete-guide-to-3d-visualization-device-system-in-r-r-software-and-data-visualization#basic-graph

print("Uncomment here to get more 3D in dynamic interactive form [WILL NOT KNIT]");
### uncomment, it will not KNIT
# rgl.open(); ## Open a new RGL device
# rgl.bg(color = "white");
# rgl.spheres(Xs.factanal.5$loadings[,1:3], 
#             r = rs, 
#             color = colors
#             ); 
### right click on a sphere
# identify3d(Xs.factanal.5$loadings[,1:3], labels = names(Xs), n = 5);


### alternatively
## uncomment, it will not KNIT
# plot3d(Xs.factanal.5$loadings[,1:3],
#         col=colors, box = FALSE,
#         type ="s", radius = rs
#       );






## TODO ... create a movie and store in directory ... create a PNG ...





## how about the raw data ...

round( psych::describe(X), digits=5);
X.factanal.5 = factanal(X, factors=5, rotation='none');
head(X.factanal.5$uniquenesses);
print(X.factanal.5$loadings, digits=2, cutoff=0.25, sort=FALSE);

plot(X.factanal.5$loadings[,1:2], type="n");
text(X.factanal.5$loadings[,1:2],labels=names(X),cex=.7) # add variable names

## notice any significant differences?  why or why not?  will this always be the case?  why is it the case in this situation?

```

# 3.5 Variance Accounted For (VAF)
The first dimension is selected to maximize explaining the data. The proportion or percentage of variance explained for that dimension is reported (% VAF).

The second dimension is selected to be orthogonal to the first dimension. The proportion or percentage of variance explained for that dimension is reported (% VAF). And the cumulative proportion or percentage of variance is also recorded (C. % VAF).

The third dimension is selected to be orthogonal to the first and second dimension. The proportion or percentage of variance explained for that dimension is reported (% VAF). And the cumulative proportion or percentage of variance is also recorded (C. % VAF).

And, so on.

```{r, how-many-factors}
source_url( paste0(path.github, "humanVerseWSU/R/functions-EDA.R") );   # how many factors

Xs.how.many = howManyFactorsToSelect(Xs);

```

# PCA

## PCA Matrix

```{r PCA-matrix}

Xs.princomp = stats::princomp(Xs);
  summary(Xs.princomp);   # , loadings=TRUE);

Xs.prcomp = stats::prcomp(Xs);
  summary(Xs.prcomp);

```

## PCA Biplot

```{r PCA-biplot}

biplot(Xs.princomp);  # equivalent to # biplot(Xs.prcomp);
biplot(Xs.princomp, 1:2);
```

